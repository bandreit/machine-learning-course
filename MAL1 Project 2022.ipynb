{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5450d0b",
   "metadata": {},
   "source": [
    "# Machine Learning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c2f32f",
   "metadata": {},
   "source": [
    "#### Student names and numbers:\n",
    "\n",
    "#####     Andrei Bostan   293104\n",
    "#####     Daniel Railean  294241\n",
    "#####     Ion Creciun     293166"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30aeca58",
   "metadata": {},
   "source": [
    "The assignments below should be solved and documented as a project that will form the basis for the\n",
    "examination. When solving the exercises it is important that you\n",
    "\n",
    "  * document all relevant results and analyses that you have obtained/performed during the exercises.\n",
    "  * try to relate your results to the theoretical background of the methods being applied.\n",
    "\n",
    "Feel free to add cells if you need to.\n",
    "\n",
    "Please hand in assignment 1-6 in a _**single**_ Jupyter notebook where you retain the questions outlined below. You are welcome to adapt code from the web (e.g. Kaggle kernels), but you **_must_** reference the original source in your notebook. In addition to _clean, well-documented code_ (i.e. functions with <a href=\"https://www.geeksforgeeks.org/python-docstrings/\">docstrings</a>, etc), your notebook will be judged according to how well each step is explained (using Markdown). \n",
    "\n",
    "In general, direct questions regarding assignments 1, 4, 5 and 6 to Frederik, and questions regarding assignments 2, 3, and 7 to Richard. \n",
    "\n",
    "Last, but not least:\n",
    "* Looking for an overview of the markdown language? The cheat sheet <a href=\"https://medium.com/ibm-data-science-experience/markdown-for-jupyter-notebooks-cheatsheet-386c05aeebed\">here</a> might help.\n",
    "* For the Python specific components of the exercises, you should not need constructs beyond those that are already included in the notebooks on the course's web-page (still you should not feel constrained by these, so feel free to be adventurous). You may, however, need to consult the documentation for some of the methods supplied by `sklearn`.\n",
    "\n",
    "**Groups:** Create your own groups. May be across teams. 2-4 students per group. No one-person groups.\n",
    "\n",
    "\n",
    "**Submission deadline:** Thursday, December 15 before 13.00 CET (Notebooks + presentation recording)\n",
    "\n",
    "**Expected workload:** Each student is expected to spend around around 50 hours on the project.\n",
    "\n",
    "### Deliverables\n",
    "The teams have to submit three deliverables before the submission deadline: 1) a notebook of assignments 1-6, 2) a notebook of assignment 7, and 3) presentation video uploaded to some online platform e.g. YouTube, Vimeo, etc.\n",
    "\n",
    "#### Notebook\n",
    "The notebook contains all the code to explore the dataset, train the final model and documents each step clearly. If code is copied from another codebase such as Github or Stack Overflow it **_must_** be properly referenced.\n",
    "\n",
    "\n",
    "#### Presentation\n",
    "The presentation video should be 15 min long and should highlight the problem you are solving, interesting things you found in the data and the step involved in building up your model. At the exam we will discuss the presentation and ask questions about your project and submissions. A link to the video must be placed in the notebook for assignment 7.\n",
    "\n",
    "### Randomness\n",
    "For ALL random states, choose state = 69 so we can replicate your work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef49835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary modules here:\n",
    "# !pip install opencv-python\n",
    "import sys\n",
    "import skimage.measure\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import imghdr\n",
    "import ast\n",
    "import json\n",
    "from PIL import Image\n",
    "import subprocess\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "!{sys.executable} -m pip install difPy\n",
    "from difPy import dif\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import (datasets, decomposition, ensemble, \n",
    "                     metrics, model_selection, preprocessing)\n",
    "from numpy.random import seed, randint\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7500606e",
   "metadata": {},
   "source": [
    "## 1. The IceCat Dataset\n",
    "\n",
    "__You should be able to do this exercise after Lecture 3.__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a32748",
   "metadata": {},
   "source": [
    "The IceCat Dataset, kindly provided to us by Stibo Systems, contains a large amount of data on different office products. As an example of \"real-world\" data, these data are imperfect and incomplete. As such, this exercise is not so much an exercise in creating a good machine learning model, but places a larger emphasis on \"cleaning the data\".\n",
    "\n",
    "We are going to work with a subset of the IceCat Dataset. In particular, you will be provided with a zip file of 5,854 images of office products, each with the name \"product ID\".jpg. You will also be provided with a list of colors, `colors.txt`, which, when imported using the code below, is a list of tuples of the form `[(\"product ID\", \"color\"), ...]`. (The code below assumes that `colors.txt` is in the same folder as the jupyter notebook. Feel free to change the code if you prefer a different organization of your files)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e77fef",
   "metadata": {},
   "source": [
    "Your task is to clean up the data and construct a simple machine learning model (_e.g._, _k_-nearest neighbor) that can identify the color of a product. You have free hands - there is hardly any one \"correct answer\" - but you need to argue for your choices. Among other things, you probably need to think about the following as you work with the data:\n",
    "\n",
    "* All of the images have different sizes.\n",
    "\n",
    "* Some of the images are RGB images (3 layers), others are CMYK (4 layers), some might even be black-and-white (1 layer).\n",
    "\n",
    "* Some colors are only represented by very few products.\n",
    "\n",
    "* Some colors are very similar, such as \"Purple\" and \"Violet\".\n",
    "\n",
    "* A product may have a particular color, but a packaging of a different color. Similarly, the color of, say, a computer monitor may be black, while the image of it could show a monitor that is turned on with a green screensaver.\n",
    "\n",
    "* Many products are attributed to several colors, such as \"Black, Blue\" or even \"Blue, Green, Orange, Violet, Yellow\". Yet others are described as \"Multicolor\" or \"Assorted colors\".\n",
    "\n",
    "Again, you have free hands in how you are going to solve these (and other) challenges, but you must argue for and reflect on your choices as you progress."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7135a9da",
   "metadata": {},
   "source": [
    "### command to install ffmpeg in PowerShell\n",
    "\n",
    "#### first install choco by using instructions from below\n",
    "\n",
    "#### using powershell and open as admin\n",
    "``` console\n",
    "Set-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString('https://community.chocolatey.org/install.ps1'))\n",
    "```\n",
    "\n",
    "#### then install ffmpeg\n",
    "``` console\n",
    "choco install ffmpeg\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d2a9e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resize all Images/ This can be skipped if you don't want to wait \n",
    "#Uncomment if you want to use this code\n",
    "\n",
    "\n",
    "# originalDirectory = 'IceCat/images/'\n",
    "# resizedDirectory = 'IceCatResized/'\n",
    "# # creates a new folder for the resized images\n",
    "# os.system('mkdir ' + resizedDirectory[:-1])\n",
    "\n",
    "# # create list of all files in directory     \n",
    "# folder_files = [filename for filename in os.listdir(originalDirectory)]  \n",
    "# for filename in folder_files: \n",
    "#     # check if the file is accesible\n",
    "#     if not os.path.isdir(originalDirectory + filename):\n",
    "#         # run ffmpeg to scale down to 100x100 preserving ratio\n",
    "#         # add padding to non-square images to make them square\n",
    "#         # adds them to the new folder\n",
    "#         os.system('ffmpeg -i ' + originalDirectory + filename  + ' -vf \"scale=w=100:h=100:force_original_aspect_ratio=1,pad=100:100:(ow-iw)/2:(oh-ih)/2:color=white\" ' + resizedDirectory  + filename )        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "365c6fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use difPy to find images that are higly similar and delete them\n",
    "#This step can be skipped as well since it takes a lot of time to process this\n",
    "#Uncomment if you want to use this code\n",
    "\n",
    "\n",
    "\n",
    "# dif(resizedDirectory, similarity=\"high\", show_progress=True, show_output=False, delete=True, silent_del=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca7170ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the images that got deleted\n",
    "originalDirectory = 'IceCat/images/'\n",
    "resizedDirectory = 'IceCatResized/'\n",
    "resizedAndDeletedDirectoru = 'IceCatResizedWoSimmilar/Images'\n",
    "\n",
    "originalImages = []\n",
    "imagesAfterDeletion = []\n",
    "deleted = []\n",
    "\n",
    "# resizedDirectory = \"IceCatResizedWoSimmilar/Images/\"\n",
    "directories = [originalDirectory, resizedDirectory]\n",
    "\n",
    "for i in range(len(directories)):\n",
    "    directory = directories[i]\n",
    "    folder_files = [filename for filename in os.listdir(directory)]  \n",
    "    for filename in folder_files:\n",
    "        if not os.path.isdir(directory + filename):\n",
    "            file_number = filename.split('.')[0]\n",
    "            if i == 0:\n",
    "                originalImages.append(int(file_number))\n",
    "            else: \n",
    "                imagesAfterDeletion.append(int(file_number))\n",
    "        \n",
    "deleted = list(set(originalImages) - set(imagesAfterDeletion))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50970296-f709-43c7-9e42-1a70ad57a2fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 files got deleted\n"
     ]
    }
   ],
   "source": [
    "print(str(len(deleted)) + ' files got deleted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc4b1e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Color\n",
      "Black     1034\n",
      "Blue       485\n",
      "Brown      179\n",
      "Green      217\n",
      "Grey       392\n",
      "Orange      89\n",
      "Pink        50\n",
      "Purple      60\n",
      "Red        276\n",
      "Silver     187\n",
      "White      694\n",
      "Yellow     218\n",
      "Name: Name, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "with open(\"IceCat/colors.txt\",\"r\") as file:\n",
    "    colors = ast.literal_eval(file.read())\n",
    "    \n",
    "# put colors and images in a dataframe\n",
    "# remove the color-image combinations that got deleted\n",
    "df = []\n",
    "for image in colors:\n",
    "    df.append(list(image))\n",
    "    \n",
    "df = pd.DataFrame(df, columns = [\"Name\",\"Color\"])\n",
    "df = df[~df[\"Name\"].isin(deleted)]\n",
    "\n",
    "# get unique colors\n",
    "# delete the color-image combinations that are represented by less then 10 images\n",
    "groups = df.groupby(\"Color\")['Name'].nunique()\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "for (index,data) in enumerate(groups):\n",
    "    if(data < 10):\n",
    "        x.append(data)\n",
    "        y.append(groups.index[index])\n",
    "        \n",
    "df = df[~df[\"Color\"].isin(y)]\n",
    "\n",
    "# delete the transparent images\n",
    "df = df[~df[\"Color\"].isin(['Transparent'])]\n",
    "df = df[~df[\"Color\"].isin(['Multicolor'])]\n",
    "\n",
    "#delete all multiple colors\n",
    "df = df[~df[\"Color\"].str.contains(\",\")]\n",
    "df = df[~df[\"Color\"].str.contains(\"Multicolour\")]\n",
    "df = df[~df[\"Color\"].str.contains(\"Assorted colours\")]\n",
    "\n",
    "\n",
    "# figure out which similar dictionary do we use\n",
    "similar = {\n",
    "    \"Brown\" : [\"Beige\",\"Wood\"],\n",
    "    \"Blue\": [\"turquoise\"],\n",
    "    \"Purple\" : [ \"Violet\"],\n",
    "    \"Silver\" : [\"Aluminium\",\"Metallic\",\"Stainless steel\"],\n",
    "    \"Yellow\" : [\"Gold\"]\n",
    "}\n",
    "\n",
    "\n",
    "# # replace colors as dictated by the similarity dictionary above\n",
    "for key in similar:\n",
    "    for color in similar[key]:\n",
    "        df.loc[df[\"Color\"].str.lower() == color.lower(), 'Color'] = key.capitalize()\n",
    "\n",
    "print(df.groupby(\"Color\")['Name'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d77a7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert images to pixel arrays\n",
    "imagesAsPixelArrays = []\n",
    "\n",
    "folder_files = [filename for filename in os.listdir(resizedDirectory)]  \n",
    "\n",
    "for filename in df[\"Name\"]:\n",
    "    im = np.array(Image.open(resizedDirectory + str(filename) + '.jpg'))\n",
    "    arr = (im.flatten())\n",
    "    imagesAsPixelArrays.append(arr)\n",
    "    \n",
    "imagesAsPixelArrays = np.array(imagesAsPixelArrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e641c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_neighbors': range(1, 10, 2)}\n",
      "2182 971\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "[CV 1/5] END .n_neighbors=1;, score=(train=0.981, test=0.606) total time=   0.8s\n",
      "[CV 2/5] END .n_neighbors=1;, score=(train=0.982, test=0.593) total time=   1.0s\n",
      "[CV 3/5] END .n_neighbors=1;, score=(train=0.980, test=0.608) total time=   1.0s\n",
      "[CV 4/5] END .n_neighbors=1;, score=(train=0.981, test=0.603) total time=   0.8s\n",
      "[CV 5/5] END .n_neighbors=1;, score=(train=0.979, test=0.601) total time=   0.9s\n",
      "[CV 1/5] END .n_neighbors=3;, score=(train=0.766, test=0.568) total time=   0.8s\n",
      "[CV 2/5] END .n_neighbors=3;, score=(train=0.762, test=0.554) total time=   0.8s\n",
      "[CV 3/5] END .n_neighbors=3;, score=(train=0.765, test=0.583) total time=   0.9s\n",
      "[CV 4/5] END .n_neighbors=3;, score=(train=0.763, test=0.573) total time=   1.0s\n",
      "[CV 5/5] END .n_neighbors=3;, score=(train=0.772, test=0.571) total time=   0.8s\n",
      "[CV 1/5] END .n_neighbors=5;, score=(train=0.698, test=0.572) total time=   0.7s\n",
      "[CV 2/5] END .n_neighbors=5;, score=(train=0.685, test=0.558) total time=   0.7s\n",
      "[CV 3/5] END .n_neighbors=5;, score=(train=0.694, test=0.557) total time=   0.8s\n",
      "[CV 4/5] END .n_neighbors=5;, score=(train=0.692, test=0.546) total time=   0.8s\n",
      "[CV 5/5] END .n_neighbors=5;, score=(train=0.684, test=0.562) total time=   0.8s\n",
      "[CV 1/5] END .n_neighbors=7;, score=(train=0.664, test=0.563) total time=   0.8s\n",
      "[CV 2/5] END .n_neighbors=7;, score=(train=0.653, test=0.547) total time=   0.8s\n",
      "[CV 3/5] END .n_neighbors=7;, score=(train=0.660, test=0.546) total time=   0.8s\n",
      "[CV 4/5] END .n_neighbors=7;, score=(train=0.656, test=0.537) total time=   0.8s\n",
      "[CV 5/5] END .n_neighbors=7;, score=(train=0.645, test=0.550) total time=   0.9s\n",
      "[CV 1/5] END .n_neighbors=9;, score=(train=0.623, test=0.572) total time=   1.0s\n",
      "[CV 2/5] END .n_neighbors=9;, score=(train=0.623, test=0.524) total time=   1.0s\n",
      "[CV 3/5] END .n_neighbors=9;, score=(train=0.631, test=0.546) total time=   0.7s\n",
      "[CV 4/5] END .n_neighbors=9;, score=(train=0.625, test=0.525) total time=   0.7s\n",
      "[CV 5/5] END .n_neighbors=9;, score=(train=0.619, test=0.546) total time=   0.7s\n",
      "Test score: 0.62\n",
      "Best parameter: {'n_neighbors': 1}\n",
      "Best cross-validation score: 0.6022022547393614\n",
      "Best estimator: KNeighborsClassifier(n_neighbors=1)\n"
     ]
    }
   ],
   "source": [
    "#MAYBE REMOVE AND USE THE PLOT STUFF\n",
    "\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# #Look only at odd numbers\n",
    "# parameters = {'n_neighbors': range(1,10,2)}\n",
    "# grid_search = GridSearchCV(KNeighborsClassifier(), parameters, cv=5, refit = True, return_train_score=True, verbose=4)\n",
    "\n",
    "# # Load the data and divide into train and test\n",
    "# X_train, X_test, y_train, y_test = train_test_split(imagesAsPixelArrays, df['Color'], random_state=69)\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, random_state=69)\n",
    "# print(parameters)\n",
    "# print(len(X_train),len(X_test))\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# print(\"Test score: {:.2f}\".format(grid_search.score(X_val, y_val)))\n",
    "\n",
    "# print(\"Best parameter: {}\".format(grid_search.best_params_))\n",
    "\n",
    "# print(\"Best cross-validation score: {}\".format(grid_search.best_score_))\n",
    "\n",
    "# print(\"Best estimator: {}\".format(grid_search.best_estimator_))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a33fb93-85c7-4c20-9650-91bba3613a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_accuracy = []\n",
    "test_accuracy = []\n",
    "# try n_neighbors from 1 to 20 only odd numbers\n",
    "neighbors_settings = range(1, 40,2)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(imagesAsPixelArrays, df['Color'], random_state=69)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, random_state=69)\n",
    "\n",
    "\n",
    "for n_neighbors in neighbors_settings:\n",
    "    # build the model\n",
    "    clf = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "    clf.fit(X_train, y_train)\n",
    "    # record training set accuracy\n",
    "    training_accuracy.append(clf.score(X_train, y_train))\n",
    "    # record generalization accuracy\n",
    "    test_accuracy.append(clf.score(X_val, y_val))\n",
    "    \n",
    "plt.plot(neighbors_settings, training_accuracy, label=\"training accuracy\")\n",
    "plt.plot(neighbors_settings, test_accuracy, label=\"Validation accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"n_neighbors\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1921f09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Prediction: 0.6626947754353804\n",
      "Test Prediction: 0.5849639546858908\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=7)\n",
    "knn.fit(X_train, y_train)\n",
    "#Add score for this \n",
    "print(\"Training Prediction: {}\".format(knn.score(X_train,y_train)))\n",
    "\n",
    "print(\"Test Prediction: {}\".format(knn.score(X_test,y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a60cbe",
   "metadata": {},
   "source": [
    "## 2. Flights Departing from NYC\n",
    "\n",
    "__You should be able to do this exercise after Lecture 4.__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef005cc",
   "metadata": {},
   "source": [
    "For this exercise we will be using the famous nycflights13 data which contains the `airlines`, `airports`, `flights`, `planes`, and `weather` datasets. Please see the documentation (`nycflights13.pdf`) for further information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4441a327",
   "metadata": {},
   "source": [
    "**(a)** Load all files as pandas dataframes and display the first 5 rows of each dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239cc0b3",
   "metadata": {},
   "source": [
    "**(b)** Convert all temperature attributes to degree Celsius. We will be using this in what follows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7968e374",
   "metadata": {},
   "source": [
    "**(c)** Using OLS, investigate if flight distance is associated with arrival delay. You should be cautious regarding negative delays."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30e7bda",
   "metadata": {},
   "source": [
    "**(d)** Using OLS, investigate if departure delay is associated with arrival delay. Again,\n",
    "   consider what to do with negative delays."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b09f482",
   "metadata": {},
   "source": [
    "**(e)** Investigate whether departure delay is associated with weather conditions\n",
    "   at the origin airport. This includes descriptives, plotting, regression modelling,\n",
    "   considering missing values etc. For regression, do OLS, Ridge, Lasso, and Elastic Net.\n",
    "   The analysis should also include seasonality trends as a \"weather condition\". You could,\n",
    "   for instance, plot the daily departure delay with the date (or monthly). What are the\n",
    "   three most important weather conditions when trying to predict departure delays?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00af31a3",
   "metadata": {},
   "source": [
    "**(f)** Is the age of the plane associated with delay? Do OLS, Ridge, Lasso, and Elastic Net."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff976833",
   "metadata": {},
   "source": [
    "**(g)** Do a principal component analysis of the weather at JFK using the following columns:\n",
    "   temp, dewp, humid, wind_dir, wind_speed, precip, visib.\n",
    "   How many principal components should be used to capture the variability in the weather data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0089a18",
   "metadata": {},
   "source": [
    "**(h)** Build regression models (OLS, Ridge, Lasso, and Elastic Net) that associates\n",
    "   an airports lattitude with weather conditions (temp, dewp, humid, wind_dir, wind_speed,\n",
    "   precip, visib). Remove all but the three most significant whether conditions and redo\n",
    "   the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3650a01d",
   "metadata": {},
   "source": [
    "**(i)** On a map, plot the airports that have flights to them where the points that represent\n",
    "   airports are relative in size to the average departure delay. You can see an example in \"airports.png\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbb7057",
   "metadata": {},
   "source": [
    " **(j)** These questions require no code.\n",
    " - Suppose you are using Ridge Regression and you notice that the training error and the validation error are almost equal and fairly high. Would you say that the model suffers from high bias or high variance? Should you increase the regularization hyperparameter or reduce it?\n",
    "\n",
    "- Why would you want to use:\n",
    "        > Ridge Regression instead of plain Linear Regression (i.e. without any regularization)?\n",
    "        > Lasso instead of Ridge Regression?\n",
    "        > Elastic Net instead of Lasso?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2a30fe",
   "metadata": {},
   "source": [
    "## 3. Clustering of Handwritten Digits\n",
    "\n",
    "__You should be able to do this exercise after Lecture 5.__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462f04ff",
   "metadata": {},
   "source": [
    "This exercise will depart from the famous MNIST dataset, and we are exploring several clustering techniques with it.. This is a \".mat\" file, in order to load this file in an ipynb you have to use loadmat() function from scipy.io. (replace my path)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dab5bedf",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'mnist-original.mat'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\io\\matlab\\_mio.py\u001b[0m in \u001b[0;36m_open_file\u001b[1;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'mnist-original'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13196\\1388709579.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mloadmat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmnist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloadmat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'mnist-original'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mmnist_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"data\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmnist_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"label\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\io\\matlab\\_mio.py\u001b[0m in \u001b[0;36mloadmat\u001b[1;34m(file_name, mdict, appendmat, **kwargs)\u001b[0m\n\u001b[0;32m    222\u001b[0m     \"\"\"\n\u001b[0;32m    223\u001b[0m     \u001b[0mvariable_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'variable_names'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m         \u001b[0mMR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmat_reader_factory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m         \u001b[0mmatfile_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMR\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvariable_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[1;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"generator didn't yield\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\io\\matlab\\_mio.py\u001b[0m in \u001b[0;36m_open_file_context\u001b[1;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m@\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopened\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;32myield\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\io\\matlab\\_mio.py\u001b[0m in \u001b[0;36m_open_file\u001b[1;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mappendmat\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfile_like\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.mat'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m                 \u001b[0mfile_like\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m'.mat'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m             raise OSError(\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'mnist-original.mat'"
     ]
    }
   ],
   "source": [
    "from scipy.io import loadmat\n",
    "mnist = loadmat('mnist-original')\n",
    "mnist_data = mnist[\"data\"].T\n",
    "mnist_label = mnist[\"label\"][0]\n",
    "import numpy as np\n",
    "print(\"Number of datapoints: {}\\n\".format(mnist_data.shape[0]))\n",
    "print(\"Number of features: {}\\n\".format(mnist_data.shape[1]))\n",
    "print(\"List of labels: {}\\n\".format(np.unique(mnist_label)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9725cef2",
   "metadata": {},
   "source": [
    "There are 70,000 images, and each image has 784 features. This is because each image is 28×28 pixels,\n",
    "and each feature simply represents one pixel’s intensity, from 0 (white) to 255 (black). Let’s take a peek at one digit from the dataset. All you need to do is grab an instance’s feature vector, reshape it to a 28×28 array, and display it using Matplotlib’s `imshow()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1eb63e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "index = 4\n",
    "print(\"Value of datapoint no. {}:\\n{}\\n\".format(index,mnist_data[index]))\n",
    "print(\"As image:\\n\")\n",
    "plt.imshow(mnist_data[index].reshape(28,28),cmap=plt.cm.gray_r)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59f8903",
   "metadata": {},
   "source": [
    "**(a)** Perform k-means clustering with k=10 on this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c15c0da",
   "metadata": {},
   "source": [
    "**(b)** Using visualization techniques analogous to what we have done in the Clustering notebook\n",
    "   for the faces data, can you determine the 'nature' of the 10 constructed clusters?\n",
    "   Do the clusters (roughly) coincide with the 10 different actual digits?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41c00cd",
   "metadata": {},
   "source": [
    "**(c)** Perform a supervised clustering evaluation using adjusted rand index.\n",
    "   Are the results stable, when you perform several random restarts of k-means?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca26824",
   "metadata": {},
   "source": [
    "**(d)** Now perform hierarchical clustering on the data.\n",
    "   (in order to improve visibility in the constructed dendrograms, you can also use a\n",
    "   much reduced dataset as constructed using sklearn.utils.resample shown below).\n",
    "   Does the visual analysis of the dendrogram indicate a natural number of clusters?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b94054",
   "metadata": {},
   "source": [
    "**(e)** Using different cluster distance metrics (ward,single,average, etc.),\n",
    "   what do the clusterings look like that are produced at the level of k=10 clusters?\n",
    "   See the Clustering notebook for the needed Python code, including the fcluster\n",
    "   method to retrieve 'plain' clusterings from the hierarchical clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1526e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_mnist_data,small_mnist_label = skl.utils.resample(mnist.data,mnist.target,n_samples=200,replace='false')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2553cf91",
   "metadata": {},
   "source": [
    "**(f)** Do a DBSCAN clustering of the small dataset. Tweak the different parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba86c41f",
   "metadata": {},
   "source": [
    "**(g)** Try to compare the different clustering methods on the MNIST dataset in the same way\n",
    "   the book does on the faces dataset on pp. 195-206."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8c50bb",
   "metadata": {},
   "source": [
    "## 4. The Local Elections\n",
    "\n",
    "__You should be able to do this exercise after Lecture 6.__\n",
    "\n",
    "In the local elections of 2021, around 100 candidates stood for election for the city council of Horsens. 83 of them represented a national party, had more than one candidate and provided answers to the <a href=\"https://www.dr.dk/nyheder/politik/kandidattest\">DR Candidate Test</a>, a test designed to help voters find out who they should vote for. In this test, the candidates answered 18 questions, which we will use as features in the following. The politicians belong to 9 parties, which will be our classes.\n",
    "\n",
    "The numpy files `X_Horsens.npy` and `Y_Horsens.npy` contains the data. `Y_Horsens.npy` contains a letter representing the party to which each candidate belongs. The following parties are represented:\n",
    "\n",
    "| Party letter | Party name | Party name (English) | Political position | Party color |\n",
    "| :-: | :-: | :-: | :-: | :-: |\n",
    "| A | Socialdemokratiet | Social Democrats | Centre-left | Red |\n",
    "| B | Radikale Venstre | Social Liberal Party | Centre-left | Indigo |\n",
    "| C | Det Konservative Folkeparti | Conservative People's Party | Right-wing | Green |\n",
    "| D | Nye Borgerlige | New Right | Far-right | Black |\n",
    "| F | Socialistisk Folkeparti | Socialist People's Party | Left-wing | Fuchsia |\n",
    "| I | Liberal Alliance | Liberal Alliance | Right-wing | Cyan |\n",
    "| O | Dansk Folkeparti | Danish People's Party | Far-right | Yellow |\n",
    "| V | Venstre | Danish Liberal Party | Centre-right | Blue |\n",
    "| Z* | Enhedslisten | Red-Green Alliance | Far-left | Dark red |\n",
    "\n",
    "*_Note that, although the party letter of Enhedslisten is actually Ø, we will here use Z to avoid any complications with the wonderful Danish letters Æ, Ø and Å. Feel free to change the Z back to an Ø if you find that it does not cause any problems._\n",
    "\n",
    "Meanwhile, `X_Horsens.npy` contains the answers to the test as numbers between -1.5 and 1.5, such that -1.5 is \"Strongly disagree\", -0.5 is \"Disagree\", 0.5 is \"Agree\" and 1.5 is \"Strongly agree\". The 18 questions concern, in order, subdivision, schools, windmills, building permits, tall buildings, housing, child care, culture, nursing homes, taxes, sports, refugees, nursing homes (again), public transportation, meat-free days, welfare, privatization, and religious minorities.\n",
    "\n",
    "Both files can be imported using `numpy.load`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cef529e",
   "metadata": {},
   "source": [
    "__(a)__ How well do you (intuitively) expect that we can predict the partisan affiliation of a candidate based on their answers to the test?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533598df",
   "metadata": {},
   "source": [
    "__(b)__ Based on the answers from all 83 candidates for the Horsens city council, perform a Principal Component Analysis with 2 principal components. Plot the results in a figure using these 2 components as the axes. Label the points with the party letter and the appropriate color."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8e83f4",
   "metadata": {},
   "source": [
    "__(c)__ Comment on the results. You may consider the following questions for inspiration: Can the political parties be separated? Can the typical distinction of \"left-wing\" and \"right-wing\" be discerned? Which of the 18 questions (features) are most important?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3423aab8",
   "metadata": {},
   "source": [
    "The number of candidates (83) is on the (very) low side when we want to do machine learning. Luckily, the neighbouring city of Databorg had no less than 8,300 candidates standing for election, with a political environment similar to that of Horsens. In the following, we will use the data from Databorg. These are stored in the numpy files `X_Databorg.npy` and `Y_Databorg.npy` in same format as the Horsens data.\n",
    "\n",
    "__(d)__ Once again, perform a Principal Component Analysis and visualize the results. Compare the results to those of the Horsens data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782a6987",
   "metadata": {},
   "source": [
    "Confident that we can predict the partisan affiliation of a politician reasonably well based on their answers to the test, we want to build a model that will allow us to distinguish between the 9 political parties. For this purpose, we split the data into a training and a validation set.\n",
    "\n",
    "__(e)__ Split the data into a training and a validation set, with appropriate fractions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14cb87d",
   "metadata": {},
   "source": [
    "First, we assume that a Naive Bayes approach is sufficient for our purposes.\n",
    "\n",
    "__(f)__ Comment on the basic assumption of the Naive Bayes approach. Is this a reasonable assumption for the problem at hand?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b1c9a1",
   "metadata": {},
   "source": [
    "__(g)__ Classify the instances of the validation set using a Naive Bayes approach. Comment on the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9f1aea",
   "metadata": {},
   "source": [
    "Assume instead that a _k_-nearest neighbour approach is sufficient for our  needs.\n",
    "\n",
    "__(h)__ Using default settings of the _k_-NN classifier, classify the instances of the validation set. Comment on the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29f6e11",
   "metadata": {},
   "source": [
    "__(i)__ Play around with different values of _k_. Decide on a \"good\" value of _k_. Comment on the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3a4ae1",
   "metadata": {},
   "source": [
    "We now try to use a decision tree instead.\n",
    "\n",
    "__(j)__ What is the _minimum_ depth of an appropriate decision tree? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c20a9a",
   "metadata": {},
   "source": [
    "__(k)__ Build a decision tree with at least the depth from above. Play around with the tree depth. Include a figure that shows some relevant measure of the performance as a function of the tree depth. Comment on any issues of over-fitting. Decide on a tree which you will keep for later use. Can you do better than the _k_-NN classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502e8734",
   "metadata": {},
   "source": [
    "__(l)__ What are the most important features? Visualize this in an appropriate way. Does it match what you would expect? Compare to the results of the PCA analysis. Do we expect them to be the same? Why/why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abf8bcd",
   "metadata": {},
   "source": [
    "We know that decision trees suffer from certain problems that may be solved by using decision forests.\n",
    "\n",
    "__(m)__ Build a decision forest. Play around with the number of trees in the forest. Decide on a forest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392aa241",
   "metadata": {},
   "source": [
    "__(n)__ Extract the most important features. Comment and compare with previously obtained results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b02c96c",
   "metadata": {},
   "source": [
    "Finally, we want to compare the models we have worked with so far (i.e., Naive Bayes, _k_-NN, decision tree and decision forest).\n",
    "\n",
    "__(o)__ Compare the results of the in terms of confusion matrices, accuracy, precision, recall, and f-score. How well can we predict the partisan affiliation of a candidate based on their answers to a test? How does this compare with your intuition? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71016df",
   "metadata": {},
   "source": [
    "## 5. Sentiment Analysis\n",
    "\n",
    "__You should be able to do this exercise after Lecture 8.__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98caa84",
   "metadata": {},
   "source": [
    "In this exercise we use the IMDb-dataset, which we will use to perform a sentiment analysis. The code below assumes that the data is placed in the same folder as this notebook. We see that the reviews are loaded as a pandas dataframe, and print the beginning of the first few reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e42ec38",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv('Sentiment Analysis/reviews.txt', header=None)\n",
    "labels = pd.read_csv('Sentiment Analysis/labels.txt', header=None)\n",
    "Y = (labels=='positive').astype(np.int_)\n",
    "\n",
    "print(reviews.head)\n",
    "print(labels.head)\n",
    "Y=to_categorical(Y, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23abdd2a",
   "metadata": {},
   "source": [
    "**(a)** Split the reviews and labels in test, train and validation sets. The train and validation sets will be used to train your model and tune hyperparameters, the test set will be saved for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250e11ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(reviews, Y, test_size=0.2, random_state=69, stratify = Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a379f1db",
   "metadata": {},
   "source": [
    "**(b)** Use the `CountVectorizer` from `sklearn.feature_extraction.text` to create a Bag-of-Words representation of the reviews. (See an example of how to do this in chapter 7 of \"Muller and Guido\"). Only use the 10,000 most frequent words (use the `max_features`-parameter of `CountVectorizer`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59dafe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "stopwords = stopwords.words(\"english\")\n",
    "stopwords.extend(['br', 'p'])\n",
    "\n",
    "vect = CountVectorizer(max_features=10000, stop_words =stopwords).fit(X_train[0])\n",
    "X_train = vect.transform(X_train[0]).toarray()\n",
    "X_test = vect.transform(X_test[0]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7258dd35",
   "metadata": {},
   "source": [
    "**(c)** Explore the representation of the reviews. How is a single word represented? How about a whole review?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0c3a38",
   "metadata": {},
   "source": [
    "##### Top 1000 words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910183cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8863d5",
   "metadata": {},
   "source": [
    "###### A word is represented as a number of appearances in a sentance, corresponding with its index from the bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a87572",
   "metadata": {},
   "source": [
    "###### This is how a review is represented -> an array of 10000 elements (the entire bag of words)  - index corresponds to the bag of words and value repesents appearece in the sentance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52ec422",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0d701a",
   "metadata": {},
   "source": [
    "###### The first word that's found in the sentance is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028244bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.where(X_train[0] > 1)\n",
    "print(vect.get_feature_names_out()[i[0][0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89caa757",
   "metadata": {},
   "source": [
    "**(d)** Train a neural network with a single hidden layer on the dataset, tuning the relevant hyperparameters to optimize accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72db1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(69)\n",
    "tf.random.set_seed(69)\n",
    "input_size = X_train[0].shape[0]\n",
    "num_classes = 2 #positive or negative\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor = 'val_accuracy', patience = 10) #early stoppage when it tends to overfit\n",
    "model = Sequential() #initialize a neural network\n",
    "#add a hidden layer. L1 regularizer (Lasso - deletes the features if their importance is low)\n",
    "model.add(Dense(units = 10, activation = 'tanh', input_dim = input_size, kernel_regularizer = regularizers.l1(0.003)))\n",
    "model.add(Dense(units = num_classes, activation = 'softmax')) #add the output layer\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', metrics = ['accuracy'])#sgd = optimizers.SGD(learning_rate = 0.1))\n",
    "history = model.fit(X_train, y_train, epochs = 200, batch_size = 100, verbose = 1, validation_split = 0.2, callbacks = [callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c15524",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(\"Learning curves\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Cross entropy loss\")\n",
    "plt.plot(history.history['loss'], label = 'train')\n",
    "plt.plot(history.history['val_loss'], label = 'valid')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6de1653",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(\"Learning curves\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.plot(history.history['accuracy'], label = 'train')\n",
    "plt.plot(history.history['val_accuracy'], label = 'valid')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925cf76f",
   "metadata": {},
   "source": [
    "**(e)** Test your sentiment-classifier on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5f6de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loss + accuracy on train data: {}\".format(model.evaluate(X_train, y_train)))\n",
    "print(\"Loss + accuracy on test data: {}\".format(model.evaluate(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d9f826",
   "metadata": {},
   "source": [
    "**(h)** Use the classifier to classify a few sentences you write yourselves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2291d696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first 3 reviews are taken from IMDB to see if the score matches the sentiment\n",
    "\n",
    "# ----- good reviews ------\n",
    "review1 = 'It is now past 1 PM and I just finished watching Francis Ford Coppola\\'s \"The Godfather\". I should probably go to bed. It\\'s late and tomorrow I have to wake up a bit early. But not early enough to postpone writing these lines. Now that I have seen it three times, the opportunity of sharing my thoughts and refreshed insights are too much of a good offer to sit on. So, bear with me. This film works so well because it takes place in an underworld in which we are so embedded that we do not even observe it. Coppola puts us straight in the smack-dab center of what is, admittedly, a society made by criminals for criminals. It is also the reason why it\\'s so welcoming. We are surrounded by its inhabitants--cold-blooded murderers, men who see crime like a 9 to 5 job masquerading as honorable men. And I do mean men. From the outside, we would only witness the horrifying, disturbing manifestations of their well-thought out actions. But it goes even deeper than that. It all revolves around the Corleone family led by Don Vito Corleone (Marlon Brando). He is the most honest of these men, sitting right on the edge. But for people like him, who do not fully embrace this world, it\\'s not easy. He avoids conflict until it is absolutely necessary. He is a man defined by moral principles. There is a scene at the beginning, in which, during his daughter\\'s wedding day, one of his associates, Luca Brasi (Lenny Montana) practices his speech that he is going to give to the Don when he meets him. The scene with these two is funny and almost adorable. I could not help but sympathize both of them only to realize that I am feeling warmth for two mobsters. Not to even mention that Lenny Montana was an actual mob hit-man and that he was actually nervous as he said that line. The more I watched the more I realized just how incredibly complex and ruthless this society is and how it has the power to corrupt anyone to come in contact with it. The best example is Corleone\\'s youngest son, Michael (Al Pacino). He returns home for his sister\\'s wedding as a war hero dressed the part with his long-time girlfriend, Kay Adams (Diane Keaton). At first, he avoids this underworld, but necessity, first-hand exposure and just its sheer devilish appealing nature draws him in. As we get further in the film, the change is shocking and every outsider who ever got close to him is tainted in one way or another. If they survive it, they are drawn in as well as we are as viewers. Inside, Coppola exposes the family to us fully, with a bold personal approach and we witness every discussion, every methodically calculated choice. Crime is done simply because it is the nature of their business, and we are put on a chair alongside them, so we easily relate. For us, they are the good guys, the rival families are the bad guys. This is the greatest feat this film managed to pull off--set apart good guys and bad guys in a world filled with bad guys. This is a film of unmatched subtlety. No other movie sustains itself as good. No other film is done with such precision, attention and completeness. There are many layers which I probably missed and maybe will never notice. But I felt them. What director Francis Ford Coppola and his partner in crime (poor choice of words, sorry) Mario Puzo did is nothing short of a timeless piece of reference cinema whose influence is not based on reinventing the wheel, but rather perfecting it to the absolute maximum. Most masterpieces are remembered for their historical contributions. \"Citizen Kane\" brought the biggest step-up to the art form, the same things did \"Gone With the Wind\" or \"2001: A Space Odyssey\". \"The Godfather\" is one of the few films that will be remembered simply because they are that good and I cannot possibly imagine a greater achievement.'\n",
    "review2 = 'Nice story, didn\\'t feel like a movie'\n",
    "review3 = 'This film was probably a fascinating piece of sci-fi writing, but as a movie it was a pretty weak effort. From the cinematography to the acting - this was a non-budget sci-fi channel original movie. Sub-career actors giving hammy performances - often skating the line of annoying. I have to admit that the basic story here is very interesting and thought provoking. I cant say it was perfectly written, but this is science fiction. I think this would work good as a play. Id also like to see this re-done with a better cast and director. In the end though - there was nothing here that gave me chills or touched me in some special way or gave me any sort of epiphany like it seems to have done for so many others... At best it held my attention until the end. Its very much average and mediocre - it will end up being the sort of thing people will reference in speculative conversations with their friends to help them feel smarter. It will end up being shown to high school and college students by that hip and cool teacher. Its very accessible and gently challenges religion, so its perfect for that. Ive definitely seen better science fiction movies - with or without action or special effects.'\n",
    "review4 = 'Such a fantastic movie'\n",
    "\n",
    "# ----- bad reviews ------\n",
    "review5 = 'This was a freaking disaster, nothing good came out of it'\n",
    "review6 = 'I didn\\'t like this movie, the could have done a better job'\n",
    "review7 = 'Can we even call this a movie?'\n",
    "review8 = 'How did anyone last enough in the cinema?'\n",
    "\n",
    "# ----- mixed feelings ------\n",
    "review9 = 'This gave me mixed feelings'\n",
    "\n",
    "test_data = []\n",
    "test_data.extend([review1.lower(), review2.lower(), review3.lower(),\n",
    "                  review4.lower(), review5.lower(), review6.lower(), \n",
    "                  review7.lower(), review8.lower(), review9.lower()])\n",
    "new_X_test = vect.transform(test_data)\n",
    "probabilities = np.array(model.predict(new_X_test))\n",
    "predictions = np.argmax(probabilities, axis = 1) #what does the model predict\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8707808",
   "metadata": {},
   "source": [
    "##### All of our categorical reviews were categorised with the right label, the mixed feelings one returned a positive label which is quite an interesting thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf13d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities[8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0780e212",
   "metadata": {},
   "source": [
    "##### The probability of the \"mixed feelings\" review returns an almost identical prediction, although it tends to be more positive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57be2d7",
   "metadata": {},
   "source": [
    "## 6. Speech Recognition\n",
    "\n",
    "__You should be able to do this exercise after Lecture 9.__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b6d770",
   "metadata": {},
   "source": [
    "In this exercise, we will work with the <a href=\"https://arxiv.org/pdf/1804.03209.pdf\">Google Speech Command Dataset</a>, which can be downloaded from <a href=\"http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz\">here</a> (note: you do not need to download the full dataset, but it will allow you to play around with the raw audiofiles). This dataset contains 105,829 one-second long audio files with utterances of 35 common words.\n",
    "\n",
    "We will use a subset of this dataset as indicated in the table below.\n",
    "\n",
    "| Word | How many? | Class # |\n",
    "| :-: | :-: | :-: |\n",
    "| Yes | 4,044 | 3 |\n",
    "| No | 3,941 | 1 |\n",
    "| Stop | 3,872 | 2 |\n",
    "| Go | 3,880 | 0 |\n",
    "\n",
    "The data is given in the files `XSound.npy` and `YSound.npy`, both of which can be imported using `numpy.load`. `XSound.npy` contains spectrograms (_e.g._, matrices with a time-axis and a frequency-axis of size 62 (time) x 65 (frequency)). `YSound.npy` contains the class number, as indicated in the table above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31986fe8",
   "metadata": {},
   "source": [
    "__(a)__ Explore and prepare the data, including splitting the data in training, validation and testing data, handling outliers, perhaps taking logarithms, etc. Data preparation is - as always - quite important. Document what you do."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08395b33",
   "metadata": {},
   "source": [
    "__(b)__ Visualize a few examples of yes's, no's, stop's and go's, so that you have a reasonable intuitive understanding of the difference between the words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace8f5f7",
   "metadata": {},
   "source": [
    "__(c)__ Train a neural network and at least one other algorithm on the data. Find a good set of hyperparameters for each model. Do you think a neural network is suitable for this kind of problem? Why/why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfbfc17",
   "metadata": {},
   "source": [
    "__(d)__ Classify instances of the validation set using your models. Comment on the results in terms of metrics you have learned in the course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149c8ead",
   "metadata": {},
   "source": [
    "__(e)__ Identify (a few) misclassified words, including what they are misclassified as. Visualize them as before, and compare with your intuitive understanding of how the words look. Do you find the misclassified examples surprising?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c67323",
   "metadata": {},
   "source": [
    "## 7. Group Assignment & Presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b00db2",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "__You should be able to start up on this exercise after Lecture 1.__\n",
    "\n",
    "*This exercise must be a group effort. That means everyone must participate in the assignment.*\n",
    "\n",
    "In this assignment you will solve a data science problem end-to-end, pretending to be recently hired data scientists in a company. To help you get started, we've prepared a checklist to guide you through the project. Here are the main steps that you will go through:\n",
    "\n",
    "1. Frame the problem and look at the big picture\n",
    "2. Get the data\n",
    "3. Explore and visualise the data to gain insights\n",
    "4. Prepare the data to better expose the underlying data patterns to machine learning algorithms\n",
    "5. Explore many different models and short-list the best ones\n",
    "6. Fine-tune your models\n",
    "7. Present your solution \n",
    "\n",
    "In each step we list a set of questions that one should have in mind when undertaking a data science project. The list is not meant to be exhaustive, but does contain a selection of the most important questions to ask. We will be available to provide assistance with each of the steps, and will allocate some part of each lesson towards working on the projects.\n",
    "\n",
    "Your group must submit a _**single**_ Jupyter notebook, structured in terms of the first 6 sections listed above (the seventh will be a video uploaded to some streaming platform, e.g. YouTube, Vimeo, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf60c0b",
   "metadata": {},
   "source": [
    "### 1. Analysis: Frame the problem and look at the big picture\n",
    "1. Find a problem/task that everyone in the group finds interesting\n",
    "2. Define the objective in business terms\n",
    "3. How should you frame the problem (supervised/unsupervised etc.)?\n",
    "4. How should performance be measured?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d1e473",
   "metadata": {},
   "source": [
    "### 2. Get the data\n",
    "1. Find and document where you can get the data from\n",
    "2. Get the data\n",
    "3. Check the size and type of data (time series, geographical etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f0cd68",
   "metadata": {},
   "source": [
    "### 3. Explore the data\n",
    "1. Create a copy of the data for explorations (sampling it down to a manageable size if necessary)\n",
    "2. Create a Jupyter notebook to keep a record of your data exploration\n",
    "3. Study each feature and its characteristics:\n",
    "    * Name\n",
    "    * Type (categorical, int/float, bounded/unbounded, text, structured, etc)\n",
    "    * Percentage of missing values\n",
    "    * Check for outliers, rounding errors etc\n",
    "4. For supervised learning tasks, identify the target(s)\n",
    "5. Visualise the data\n",
    "6. Study the correlations between features\n",
    "7. Identify the promising transformations you may want to apply (e.g. convert skewed targets to normal via a log transformation)\n",
    "8. Document what you have learned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9efdb64",
   "metadata": {},
   "source": [
    "### 4. Prepare the data\n",
    "Notes:\n",
    "* Work on copies of the data (keep the original dataset intact).\n",
    "* Write functions for all data transformations you apply, for three reasons:\n",
    "    * So you can easily prepare the data the next time you run your code\n",
    "    * So you can apply these transformations in future projects\n",
    "    * To clean and prepare the test set\n",
    "    \n",
    "    \n",
    "1. Data cleaning:\n",
    "    * Fix or remove outliers (or keep them)\n",
    "    * Fill in missing values (e.g. with zero, mean, median, regression ...) or drop their rows (or columns)\n",
    "2. Feature selection (optional):\n",
    "    * Drop the features that provide no useful information for the task (e.g. a customer ID is usually useless for modelling).\n",
    "3. Feature engineering, where appropriate:\n",
    "    * Discretize continuous features\n",
    "    * Use one-hot encoding if/when relevant\n",
    "    * Add promising transformations of features (e.g. $\\log(x)$, $\\sqrt{x}$, $x^2$, etc)\n",
    "    * Aggregate features into promising new features\n",
    "4. Feature scaling: standardise or normalise features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa4ddcf",
   "metadata": {},
   "source": [
    "### 5. Short-list promising models\n",
    "We expect you to do some additional research and train at **least one model per team member**.\n",
    "\n",
    "1. Train mainly quick and dirty models from different categories (e.g. linear, SVM, Random Forests etc) using default parameters\n",
    "2. Measure and compare their performance\n",
    "3. Analyse the most significant variables for each algorithm\n",
    "4. Analyse the types of errors the models make\n",
    "5. Have a quick round of feature selection and engineering if necessary\n",
    "6. Have one or two more quick iterations of the five previous steps\n",
    "7. Short-list the top three to five most promising models, preferring models that make different types of errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b039fb8",
   "metadata": {},
   "source": [
    "### 6. Fine-tune the system\n",
    "1. Fine-tune the hyperparameters\n",
    "2. Once you are confident about your final model, measure its performance on the test set to estimate the generalisation error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec49c9a",
   "metadata": {},
   "source": [
    "### 7. Present your solution\n",
    "1. Document what you have done\n",
    "2. Create a nice 15 minute video presentation with slides\n",
    "    * Make sure you highlight the big picture first\n",
    "3. Explain why your solution achieves the business objective\n",
    "4. Don't forget to present interesting points you noticed along the way:\n",
    "    * Describe what worked and what did not\n",
    "    * List your assumptions and you model's limitations\n",
    "5. Ensure your key findings are communicated through nice visualisations or easy-to-remember statements (e.g. \"the median income is the number-one predictor of housing prices\")\n",
    "6. Upload the presentation to some online platform, e.g. YouTube or Vimeo, and supply a link to the video in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f033c3e",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8e3fdf",
   "metadata": {},
   "source": [
    "Géron, A. 2017, *Hands-On Machine Learning with Scikit-Learn and Tensorflow*, Appendix B, O'Reilly Media, Inc., Sebastopol."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
