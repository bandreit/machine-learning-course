{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Analysis: Frame the problem and look at the big picture\n",
    "\n",
    "For the 7th assignment our group decided to partake in Ion's bachelor project idea, as we found it to be interesting and we could help him achieve his goals for BPR.<br>\n",
    "The objective put into simple terms is to: create an interpreter for the American Sign Language which can be used for online direct communication, or teaching purposes. It would benefit the communication between deaf and hearing communities. <br>\n",
    "The problem would be supervised, as there are existing labels which represent words translated from the sign-language videos.<br>\n",
    "The performance would be measured in precision of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Get the data\n",
    "\n",
    "The data set can be found on [Keggle - WLASL (World Level American Sign Language) Video](https://www.kaggle.com/datasets/risangbaskoro/wlasl-processed). <br>\n",
    "At first glance, the dataset looks pretty promising. <br>\n",
    "IT contains a lot of video metadata and is very specific to a certain video library that needs to get downloaded from a different source - hosted [here](https://drive.google.com/file/d/11eFE_quM2_2-h3H_zTTjq0i0D6pkx62Z/view). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Explore the data\n",
    "After a copy of the data was downloaded, we inspected its content. <br>\n",
    "That is the moment we found out that half of the videos mentioned in the data set were missing, so we requested a new copy of all the videos from the original source. This time we got all the videos.<br>\n",
    "The features in this case are just videos, more precisely .mp4 videos. Of course processing them would come a bit later, but at this point tu a simple user they are just videos.<br>\n",
    "The targets are simply the words that each of the video tries to demonstrate, meaning that if the video is labeled with the word \"friend\", then it will simply represent a person showing the specific sign in American Sign Language for the word \"friend\" <br>\n",
    "Visualizing the video would simple mean watching the video, or representing the number of samples for each word. Even though there are not a lot of samples, a key point is that each of them are <br>\n",
    "There is no transformations that would need to be applied to the data as of this point in time, because the main idea behind it is that a third-party library from Google [MediaPipe](https://mediapipe.dev/) would add a layer on top of the videos, tracking the objects (hands, movements, faces). Further on, the real features would be given by MediaPipe, and they do not need any transformation or scaling whatsoever. <br>\n",
    "A visualization of how MediaPipe works is available below: <br> \n",
    "<br>\n",
    " ![alt text](https://mediapipe.dev/assets/img/photos/pose_1.jpg)\n",
    "<br>\n",
    "MediaPipe can be fired up on different sources, such as live video - webcam, or pre-recorder video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the needed imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from sklearn import (datasets, decomposition, ensemble, \n",
    "                     metrics, model_selection, preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_hands = mp.solutions.hands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Prepare the data\n",
    "\n",
    "First steps in preparing the data is using MediaPipe to extract the landmarks. MediaPipe detects 33 body landmarks in total, but given that the Sign Language is only expressed through upper-body and hand movements, we scraped a lot of the unimportant landmarks and used the 12 most important ones from the body in this case, represented below: <br> <br> ![body landmarks](mediapipe_landmarks.jpg) <br> <br>\n",
    "As the details of specific words in the sign language can be found in the hand and finger movement, all of the landmarks that MediaPipe offers were used, a representation can be seen below: <br> <br> ![body landmarks](hand_landmarks.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, holistic,hands):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR to RGB\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    holistic_model = holistic.process(image)                 # Make prediction\n",
    "    hands_model = hands.process(image)\n",
    "    image.flags.writeable = True                   # Image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    \n",
    "    resize_up = cv2.resize(image, (1024,800), interpolation= cv2.INTER_AREA)\n",
    "\n",
    "    return resize_up, holistic_model,hands_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an optional method which is used to draw the landmarks on top of the images that we might feed into the model. It can be used for live demos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, pose,hands):\n",
    "    if(pose.pose_world_landmarks):\n",
    "        mp_drawing.draw_landmarks(image, pose.pose_world_landmarks, mp_holistic.POSE_CONNECTIONS) # Draw pose connections\n",
    "            # Draw pose connections\n",
    "        mp_drawing.draw_landmarks(image, pose.pose_world_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    if hands.multi_hand_landmarks:\n",
    "        for hand_landmarks in hands.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS) # Draw hands\n",
    "                # Draw right hand connections  \n",
    "            mp_drawing.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We had multiple options for extracting data from the video using MediaPipe. First was using holistic solution, which allows the extraction of pose, left hand and right hand marks. In this case hand tracking allows the extraction of X, Y and Z coordinates, and works fairly well, while the pose one allows only for X and Y coordinates, Z not being fully precise enough in the model that's offered by MediaPipe. Using this option meant that the origin was in the lower-left extreme of the image - further normalized to [0.0, 1.0]. The second option was using the pose_world landmark which allowed taking the real world 3-D coordinates in meters with the origin being in the center between hips, and the hands tracked by the multi_hand_world_landmarks property, where the origin of the hands is at the base of the palms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function detects the 12 body landmarks that we talked about above, getting their X and Y coordinates. It returns NaN if they are not detected or the landmarks as an array when detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pose_world_to_array(hollistic):\n",
    "    landmarks = hollistic.pose_world_landmarks\n",
    "    data = []\n",
    "    \n",
    "    if(landmarks == None):\n",
    "        return None\n",
    "    else:\n",
    "        landmarks = landmarks.landmark[11:23]\n",
    "        for item in landmarks:\n",
    "            if(item==None):\n",
    "                data.append(None)\n",
    "            else:\n",
    "                landmark = {'X':item.x,'Y':item.y}\n",
    "                data.append(landmark)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function detects all 21 hand landmarks, splitting them by each hand (right or left). Every hand returns an array of coordinates (nan or values) same as for the pose. If there is only one hand detected, or if both hands are detected but they are in the same position (i.e. both are left or both are right), the code determines the landmarks of the hand using the multi_hand_world_landmarks property of the hands input. The landmarks are then returned as a list of dictionaries with the 3D coordinates of the landmarks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_left_and_right_hand_landmarks( hands):\n",
    "    hands_position = hands.multi_handedness\n",
    "    hands_landmarks = hands.multi_hand_world_landmarks\n",
    "    \n",
    "    if not hands_position:\n",
    "        return [None, None]\n",
    "        \n",
    "    second_hand=None\n",
    "\n",
    "    first_hand = hands_position[0].classification[0]\n",
    "    if(len(hands_position) == 2):\n",
    "        second_hand = hands_position[1].classification[0]\n",
    "\n",
    "    if len(hands_position) == 1 or (second_hand and first_hand.label == second_hand.label):\n",
    "        if(len(hands_position) == 2 and first_hand.score < second_hand.score):\n",
    "            landmarks = hands_landmarks[1].landmark\n",
    "        else:\n",
    "            landmarks = hands_landmarks[0].landmark\n",
    "        hand = []\n",
    "        for item in landmarks:\n",
    "            hand.append({'X': item.x, 'Y': item.y, 'Z': item.z})\n",
    "\n",
    "        if hands_position[0].classification[0].label == 'Left':\n",
    "            return [None,hand]\n",
    "\n",
    "        return [hand,None]\n",
    "\n",
    "    left_hand = []\n",
    "    right_hand = []\n",
    "    left_hand_landmarks = hands_landmarks[0].landmark\n",
    "    right_hand_landmarks = hands_landmarks[1].landmark\n",
    "    for i in range(len(left_hand_landmarks)):\n",
    "        right_hand.append({'X': left_hand_landmarks[i].x, 'Y': left_hand_landmarks[i].y, 'Z': left_hand_landmarks[i].z})\n",
    "        left_hand.append({'X': right_hand_landmarks[i].x, 'Y': right_hand_landmarks[i].y, 'Z': right_hand_landmarks[i].z})\n",
    "\n",
    "    return [left_hand, right_hand]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the top 5 words that we use for this model's predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json(\"Data.json\")\n",
    "top_words = ['thank you','love','deaf',\"hello\",\"friend\"]\n",
    "\n",
    "X_thank_you = data[data['label']==\"thank you\"]\n",
    "X_love= data[data['label']==\"love\"]\n",
    "X_deaf = data[data['label']==\"deaf\"]\n",
    "X_hello = data[data['label']==\"hello\"]\n",
    "X_friend = data[data['label']==\"friend\"]\n",
    "\n",
    "data_type = {\"RIGHT_HAND_WORLD_LANDMARKS\": [], \"LEFT_HAND_WORLD_LANDMARKS\":[],\"POSE_WORLD_LANDMARKS\":[],\"Label\":[]}\n",
    "X = np.concatenate((X_thank_you,X_love,X_deaf,X_hello,X_friend))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is processing a dataset of videos by extracting information about hand and body pose. The code iterates over a list of videos, indicated by the variable X, and extracts pose data using the mediapipe_detection function. The pose data is then converted and appended to the data_type dictionary, which is likely used to store the extracted data. The code also includes a check to see if the video capture was successful, and breaks the loop if not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Folder = \"C:/Users/ion/Desktop/WLASL ML PROCESSING/WLASL2000/WLASL2000/\"\n",
    "index = 0\n",
    "for item in X:\n",
    "    print(item)\n",
    "    print(Folder + str(item[1]).zfill(5) + \".mp4\")\n",
    "    cap = cv2.VideoCapture(Folder + str(item[1]).zfill(5) + \".mp4\")\n",
    "    right_hand = []\n",
    "    left_hand = []\n",
    "    pose_shape = []\n",
    "    pose_world = []\n",
    "    right_hand_world = []\n",
    "    left_hand_world = []\n",
    "    data_type[\"Label\"].append(item[0].lower())\n",
    "    print(index)\n",
    "    index = index+1\n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5,min_tracking_confidence=0.5) as pose:\n",
    "        with mp_hands.Hands(min_detection_confidence=0.3,min_tracking_confidence=0.3) as hands:\n",
    "            while(cap.isOpened()):\n",
    "                success, frame = cap.read()\n",
    "                if(not success):\n",
    "                    break\n",
    "                if(success):\n",
    "                    frame.flags.writeable = False\n",
    "                    image,hollistic,hands_position = mediapipe_detection(frame,pose,hands)\n",
    "                    pose_world.append(convert_pose_world_to_array(hollistic))\n",
    "\n",
    "                    hands_world_landmarks = get_left_and_right_hand_landmarks(hands_position)\n",
    "\n",
    "                    left_hand_world.append(hands_world_landmarks[0])\n",
    "                    right_hand_world.append(hands_world_landmarks[1])\n",
    "                    \n",
    "    data_type[\"POSE_WORLD_LANDMARKS\"].append(pose_world)   \n",
    "    data_type[\"LEFT_HAND_WORLD_LANDMARKS\"].append(left_hand_world)\n",
    "    data_type[\"RIGHT_HAND_WORLD_LANDMARKS\"].append(right_hand_world)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame.from_dict(data_type)\n",
    "# df.to_json('5_words_data.json',orient='index')\n",
    "df = pd.read_json('5_words_data.json',orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As if data consistency is very important for the model, all the videos should be transformed into 60 frames total, resulting in a 30fps. There were a lot of different frames per video. In the cases were a video had more frames, then we checked the hands, if both of them are none - we remove the frame, otherwise, we just crop and select the first 60 frames. If the video has less than 60 frames in total, then we just add NaN to the end of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_removed_index_from_world_landmarks(right_hand,left_hand):\n",
    "    removed_indexes = []\n",
    "    # print((right_hand)==None)\n",
    "    for i in range(len(right_hand)):\n",
    "        if(len(right_hand) - len(removed_indexes) <=60):\n",
    "            break\n",
    "        if(right_hand[i] == None and left_hand[i] == None):\n",
    "            removed_indexes.append(i)\n",
    "    return removed_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_60_frames(data):\n",
    "    for item in data.iterrows():\n",
    "        world_remove_indexes = get_removed_index_from_world_landmarks(item[1].RIGHT_HAND_WORLD_LANDMARKS,item[1].LEFT_HAND_WORLD_LANDMARKS)\n",
    "\n",
    "        if(len(item[1].RIGHT_HAND_WORLD_LANDMARKS)<60):\n",
    "            current_length = len(item[1].RIGHT_HAND_WORLD_LANDMARKS)\n",
    "            for i in range(current_length,60):\n",
    "                item[1].RIGHT_HAND_WORLD_LANDMARKS.append(None)\n",
    "                item[1].LEFT_HAND_WORLD_LANDMARKS.append(None)\n",
    "                item[1].POSE_WORLD_LANDMARKS.append(None)\n",
    "\n",
    "        \n",
    "        for index in sorted(world_remove_indexes, reverse=True):\n",
    "            del item[1].RIGHT_HAND_WORLD_LANDMARKS[index]\n",
    "            del item[1].LEFT_HAND_WORLD_LANDMARKS[index]\n",
    "            del item[1].POSE_WORLD_LANDMARKS[index]\n",
    "            \n",
    "        if(len(item[1].RIGHT_HAND_WORLD_LANDMARKS)>60):\n",
    "            del item[1].RIGHT_HAND_WORLD_LANDMARKS[60:]\n",
    "            del item[1].LEFT_HAND_WORLD_LANDMARKS[60:]\n",
    "            del item[1].POSE_WORLD_LANDMARKS[60:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions are used to convert the coordinates (X, Y, Z) from pose and hands to the 60x150 array were 60 is the number of frames and 150 is the total number of points from both hands and the pose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = []\n",
    "labels = []\n",
    "def extract_keypoints(pose_world_landmarks,right_hand_world_landmarks,left_hand_world_landmarks):\n",
    "    pose = np.array([[res['X'], res['Y']] for res in pose_world_landmarks]).flatten() if pose_world_landmarks[0] else np.zeros(12*2)\n",
    "    lh = np.array([[res['X'], res['Y'], res['Z']] for res in left_hand_world_landmarks ]).flatten() if left_hand_world_landmarks  else np.zeros(21*3)\n",
    "    rh = np.array([[res['X'], res['Y'], res['Z']] for res in right_hand_world_landmarks ]).flatten() if right_hand_world_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose, lh, rh])\n",
    "\n",
    "def convert_world_to_2d_array(data):\n",
    "    k = 0\n",
    "    for item in data.iterrows():\n",
    "        sequence = []\n",
    "        labels.append(item[1].Label)\n",
    "        for i in range(len(item[1].RIGHT_HAND_WORLD_LANDMARKS)):    \n",
    "            pose_world = item[1].POSE_WORLD_LANDMARKS[i]\n",
    "            if pose_world == None:\n",
    "                pose_world = [None]\n",
    "            points = extract_keypoints(pose_world,item[1].RIGHT_HAND_WORLD_LANDMARKS[i],item[1].LEFT_HAND_WORLD_LANDMARKS[i])\n",
    "            sequence.append(points)\n",
    "        sequences.append(sequence)\n",
    "        k= k+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_60_frames(df)\n",
    "convert_world_to_2d_array(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot encoding the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = np.array(sequences,dtype=float)\n",
    "words = np.array(labels).reshape(-1,1)\n",
    "print(words)\n",
    "Y = OneHotEncoder().fit_transform(words).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the random seed to 69"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_random_seeds():\n",
    "   tf.random.set_seed(69)\n",
    "   np.random.seed(69)\n",
    "   random.seed(69)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape,Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting data into tran and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test , y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=69,stratify = Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Short-list promising models\n",
    "###### and\n",
    "### 6. Fine-tune the system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_random_seeds()\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "# input layer\n",
    "model.add(LSTM(128, return_sequences=True, activation='relu', input_shape=(60,150)))\n",
    "\n",
    "#hidden layers\n",
    "model.add(LSTM(256, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(256, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(128, return_sequences=False, activation='relu'))\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "\n",
    "#output layer\n",
    "model.add(Dense(Y.shape[1], activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=430,batch_size=10,validation_split = 0.2,verbose = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Learning curves\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.plot(history.history['accuracy'], label = 'train')\n",
    "plt.plot(history.history['val_accuracy'], label = 'valid')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy on training data: {}\".format(model.evaluate(X_train, y_train)))\n",
    "print(\"Accuracy on test data: {}\".format(model.evaluate(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "top_words = np.array(['thank you', 'love', 'deaf', 'hello','friend'])\n",
    "actual =  np.take(top_words,[np.argmax(pred) for pred in y_test])\n",
    "predicted = np.take(top_words,[np.argmax(pred) for pred in model.predict(X_test)])\n",
    "confusion_matrix = metrics.confusion_matrix(actual, predicted)\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = ['thank you', 'love', 'deaf', 'hello','friend'])\n",
    "\n",
    "cm_display.plot()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "Accuracy = metrics.accuracy_score(actual, predicted)\n",
    "Precision = metrics.precision_score(actual, predicted, average=\"weighted\")\n",
    "Sensitivity_recall = metrics.recall_score(actual, predicted, average=\"weighted\")\n",
    "F1_score = metrics.f1_score(actual, predicted, average=\"weighted\")\n",
    "\n",
    "print(\"Accuracy measures how often the model is correct: {}\".format(Accuracy))\n",
    "print(\"Precision measures percentage of true positive: {}\".format(Precision))\n",
    "print(\"Sensitivity is good at understanding how well the model predicts something is positive: {}\".format(Sensitivity_recall))\n",
    "print(\"F-score is the harmonic mean of precision and sensitivity. It considers both false positive and false negative cases: {}\".format(F1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we had to build more than one model, we decided to go with a KNN algorithm and a Random Forrest.\n",
    "To get a better working models, we used a GridSearch, which takes up different possible parameters to construct the model and runs through all of them to find out the best ones. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-nearest neighbors (KNN) is a simple and effective method for classification and regression. In the context of predicting sign language from videos, KNN could potentially be used because it is a non-parametric method that does not make any assumptions about the underlying data distribution. This means that it can model complex, non-linear relationships between the input data (the videos) and the output labels (the predicted sign language gestures). Additionally, KNN is relatively easy to implement and can be used for multi-class classification, which would be necessary in this case where there are multiple possible sign language gestures to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tuning the hyperparameters in the K-nearest neighbors (KNN) algorithm. The n_neighbors parameter specifies the number of data points used to make predictions for a given data point. The metric parameter specifies the method used to measure the distance between data points. Choosing the right values for these parameters can be important for achieving good results with the KNN algorithm. that is why the number of neighbors might go up to 15 in this case, and minkowski, euclidean and manhattan are all possible good metrics that can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "X_reshaped= X.reshape(X.shape[0], (X.shape[1]*X.shape[2]))\n",
    "Y_reshaped = words.flatten()\n",
    "X_train, X_test , y_train, y_test = train_test_split(X_reshaped, Y_reshaped, test_size=0.2, random_state=69,stratify = Y_reshaped)\n",
    "\n",
    "parameters = {'n_neighbors':range(1,15,2),'weights' : ['uniform','distance'],\n",
    "               'metric' : ['minkowski','euclidean','manhattan'],\n",
    "               'algorithm':['ball_tree','kd_tree','brute']}\n",
    "\n",
    "# X_R.shape,words.shape\n",
    "grid_search = GridSearchCV(KNeighborsClassifier(),parameters,cv=5,refit=True,return_train_score=True,verbose = 4)\n",
    "\n",
    "\n",
    "grid_search.fit(X_train,y_train)\n",
    "\n",
    "print(\"Best Parameter : {}\".format(grid_search.best_params_))\n",
    "print(\"Best Cross Validation Score : {}\".format(grid_search.best_score_))\n",
    "print(\"Best estimator : {}\".format(grid_search.best_estimator_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Test Score : {}\".format(grid_search.score(X_test_knn,Y_test_knn)))\n",
    "knn = KNeighborsClassifier(algorithm= 'ball_tree', metric =  'manhattan', n_neighbors= 3, weights= 'distance')\n",
    "\n",
    "kNN_y_pred = knn.fit(X_train, y_train).predict(X_test)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = np.array(['thank you', 'love', 'deaf', 'hello','friend'])\n",
    "actual = y_test\n",
    "predicted = kNN_y_pred\n",
    "\n",
    "confusion_matrix = metrics.confusion_matrix(actual, predicted)\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = ['thank you', 'love', 'deaf', 'hello','friend'])\n",
    "\n",
    "cm_display.plot()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "Accuracy = metrics.accuracy_score(actual, predicted)\n",
    "Precision = metrics.precision_score(actual, predicted, average=\"weighted\")\n",
    "Sensitivity_recall = metrics.recall_score(actual, predicted, average=\"weighted\")\n",
    "F1_score = metrics.f1_score(actual, predicted, average=\"weighted\")\n",
    "\n",
    "print(\"Accuracy measures how often the model is correct: {}\".format(Accuracy))\n",
    "print(\"Precision measures percentage of true positive: {}\".format(Precision))\n",
    "print(\"Sensitivity is good at understanding how well the model predicts something is positive: {}\".format(Sensitivity_recall))\n",
    "print(\"F-score is the harmonic mean of precision and sensitivity. It considers both false positive and false negative cases: {}\".format(F1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forests are a type of machine learning algorithm that can be used for a variety of tasks, including video processing. One reason to use a random forest for video processing is that they can handle a large number of input variables, which is useful when working with high-dimensional data like video frames. Additionally, random forests are able to learn complex nonlinear relationships between input variables and the output, which can be useful for tasks like object recognition in video. Furthermore, random forests are relatively simple to implement and interpret, which can be helpful when working with video data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tuning the hyperparams for random-forests: n_estimators specifies the number of trees in the random forest. Criterion specifies the function used to measure the quality of a split in the decision trees. Max_features specifies the maximum number of features considered when splitting a node. Min_samples_leaf specifies the minimum number of samples required to be at a leaf node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'n_estimators':range(100, 200, 10), 'criterion': ['gini','entropy'], 'max_features': ['log2', 'sqrt'], 'min_samples_leaf': range(5, 10), }  \n",
    "grid_search = GridSearchCV(RandomForestClassifier(),parameters,cv=5,refit=True,return_train_score=True,verbose = 4)  \n",
    "grid_search.fit(X_train,y_train)  \n",
    "  \n",
    "print(\"Best Parameter : {}\".format(grid_search.best_params_))  \n",
    "print(\"Best Cross Validation Score : {}\".format(grid_search.best_score_))  \n",
    "print(\"Best estimator : {}\".format(grid_search.best_estimator_))\n",
    "rf = RandomForestClassifier(criterion='entropy', min_samples_leaf=5,random_state=69) \n",
    "rf_pred = rf.fit(X_train,y_train).predict(X_test)\n",
    "top_words = np.array(['thank you', 'love', 'deaf', 'hello','friend']) \n",
    "actual = y_test \n",
    "predicted = rf_pred \n",
    " \n",
    "confusion_matrix = metrics.confusion_matrix(actual, predicted) \n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = ['thank you', 'love', 'deaf', 'hello','friend']) \n",
    " \n",
    "cm_display.plot() \n",
    "plt.show() \n",
    " \n",
    " \n",
    "Accuracy = metrics.accuracy_score(actual, predicted) \n",
    "Precision = metrics.precision_score(actual, predicted, average=\"weighted\") \n",
    "Sensitivity_recall = metrics.recall_score(actual, predicted, average=\"weighted\") \n",
    "F1_score = metrics.f1_score(actual, predicted, average=\"weighted\") \n",
    " \n",
    "print(\"Accuracy measures how often the model is correct: {}\".format(Accuracy)) \n",
    "print(\"Precision measures percentage of true positive: {}\".format(Precision)) \n",
    "print(\"Sensitivity is good at understanding how well the model predicts something is positive: {}\".format(Sensitivity_recall)) \n",
    "print(\"F-score is the harmonic mean of precision and sensitivity. It considers both false positive and false negative cases: {}\".format(F1_score))v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
